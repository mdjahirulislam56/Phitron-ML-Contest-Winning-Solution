# Phitron ML Contest 01 - Phitron-ML-Contest-Winning-Solution

![Rank](https://img.shields.io/badge/Rank-1st_Place-gold)
![Language](https://img.shields.io/badge/Python-3.12-blue)
![Model](https://img.shields.io/badge/Model-CatBoost-green)

## üìñ Overview
This repository contains the solution that secured **1st Place** in the private contest: **Phitron ML Contest 01** hosted on Kaggle. The challenge focused on predicting the age of crabs based on physical measurements such as length, diameter, height, and weight components.

My approach utilized a **CatBoostRegressor** integrated into a Scikit-Learn pipeline, emphasizing robust feature engineering to capture non-linear geometric relationships.

## üèÜ Achievement
**Rank:** 1st Place  
<img width="1506" height="861" alt="image" src="https://github.com/user-attachments/assets/cef7269f-5ac9-4156-9487-8b72729f7e2b" />

## üìä Methodology

### 1. Feature Engineering
A custom `FeatureEngineer` class was developed to generate domain-specific features, which was a key factor in the model's performance:
* **Geometric Features:** Calculated `Volume`, `Surface_Area`, `L2_Dim` (Euclidean magnitude), and `Flatness` (Height/Length ratio).
* **Weight Decomposition:** Derived `Water_Weight` (Total Weight - Component Weights) and `Density`.
* **Ratios:** Created relative metrics like `Meat_Ratio`, `Shell_Ratio`, and `Shell_to_Body` ratio.
* **Transformations:** Applied `log1p` transformations to all weight variables to reduce skewness.

### 2. Preprocessing
* **Categorical:** One-Hot Encoding for the `Sex` feature.
* **Numerical:** RobustScaler was used to handle outliers in physical dimensions.

### 3. Model Configuration
The final model was a **CatBoostRegressor** with the following hyperparameters:
* **Loss Function:** MAE (Mean Absolute Error)
* **Iterations:** 800
* **Depth:** 7
* **Learning Rate:** 0.02
* **Regularization:** L2 Leaf Reg = 10

### 4. Validation
* **Strategy:** 5-Fold Cross-Validation with shuffling.
* **Inference:** Final predictions were generated by averaging the outputs from all 5 folds and rounding to the nearest integer.

## üìÇ Project Structure
* `Best_Train.ipynb`: The Jupyter Notebook containing the full training pipeline, feature extraction, and inference logic.
* `requirements.txt`: Python dependencies required to run the code.
* `train.csv`: Training dataset.
* `test.csv`: Testing dataset.

## üöÄ How to Run
1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/mdjahirulislam56/Phitron-ML-Contest-Winning-Solution.git](https://github.com/mdjahirulislam56/Phitron-ML-Contest-Winning-Solution.git)
    cd Phitron-ML-Contest-Winning-Solution
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Run the Notebook:**
    Open `Best_Train.ipynb` in Jupyter or Google Colab. Ensure the dataset files (`train.csv`, `test.csv`) are accessible.

## üìù Author
**Md Jahirul Islam**

---
*This solution demonstrates the effectiveness of feature engineering in regression tasks.*
